{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.Tensor([2.0]).double()\n",
    "x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()\n",
    "x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()\n",
    "w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()\n",
    "w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()\n",
    "b.requires_grad = True\n",
    "n = x1 * w1 + x2 * w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print(\"---\")\n",
    "print(\"x2\", x2.grad.item())\n",
    "print(\"w2\", w2.grad.item())\n",
    "print(\"x1\", x1.grad.item())\n",
    "print(\"w1\", w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(\n",
    "            other, (int, float)\n",
    "        ), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f\"**{other}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __radd__(self, other):  # other + self\n",
    "        return self + other\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (\n",
    "                out.data * out.grad\n",
    "            )  # NOTE: in the video I incorrectly used = instead of +=. Fixed here.\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "\n",
    "class MLP:\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i + 1]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = MLP(3, [4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.3131266839671677),\n",
       " Value(data=0.6128597978704488),\n",
       " Value(data=0.6722930800037457),\n",
       " Value(data=-0.04865731120282704)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # desired targets\n",
    "\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=3.9926988685725746)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum([(yout - ygt) ** 2 for ygt, yout in zip(ys, ypred)])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10242391349254651"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8890348950249369"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.layers[0].neurons[0].w[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.221864717500871\n",
      "1 5.394210357115138\n",
      "2 4.671158017127025\n",
      "3 4.868320370520471\n",
      "4 4.8150236247546685\n",
      "5 4.704810414129374\n",
      "6 4.578329130852987\n",
      "7 4.436617802080616\n",
      "8 3.9108079113854872\n",
      "9 3.39719262927922\n",
      "10 1.4052404307461457\n",
      "11 0.18113900796003515\n",
      "12 0.09093616643415664\n",
      "13 0.067126321453075\n",
      "14 0.0548353254795098\n",
      "15 0.0469998902727582\n",
      "16 0.04141622484659291\n",
      "17 0.037158041845535214\n",
      "18 0.03376235775516737\n",
      "19 0.030968788855938813\n",
      "20 0.02861766261299463\n",
      "21 0.026604386547928422\n",
      "22 0.024856826675578092\n",
      "23 0.023323159200144133\n",
      "24 0.02196490760591896\n",
      "25 0.020752729614198044\n",
      "26 0.01966374880883197\n",
      "27 0.01867979757320247\n",
      "28 0.017786220913778177\n",
      "29 0.016971038426909578\n",
      "30 0.016224342421970774\n",
      "31 0.015537856220061274\n",
      "32 0.014904603827440653\n",
      "33 0.014318658771044455\n",
      "34 0.013774950307164037\n",
      "35 0.013269111939008541\n",
      "36 0.012797361621416068\n",
      "37 0.012356406029718813\n",
      "38 0.011943363333827114\n",
      "39 0.011555700364747528\n",
      "40 0.011191181090479976\n",
      "41 0.010847824062361129\n",
      "42 0.010523867037958157\n",
      "43 0.01021773739080385\n",
      "44 0.009928027220431655\n",
      "45 0.009653472305959391\n",
      "46 0.009392934222337595\n",
      "47 0.00914538507419082\n",
      "48 0.008909894407931697\n",
      "49 0.008685617945820378\n",
      "50 0.008471787851235521\n",
      "51 0.008267704286627521\n",
      "52 0.00807272806742802\n",
      "53 0.00788627424886954\n",
      "54 0.007707806509953405\n",
      "55 0.0075368322210261265\n",
      "56 0.007372898099612989\n",
      "57 0.00721558637411443\n",
      "58 0.007064511387328646\n",
      "59 0.006919316582013354\n",
      "60 0.006779671819238861\n",
      "61 0.006645270987424458\n",
      "62 0.006515829865941847\n",
      "63 0.006391084212216984\n",
      "64 0.006270788045526322\n",
      "65 0.006154712104299106\n",
      "66 0.00604264245681324\n",
      "67 0.00593437924779409\n",
      "68 0.005829735565670282\n",
      "69 0.005728536417164439\n",
      "70 0.0056306177975519464\n",
      "71 0.005535825846348335\n",
      "72 0.005444016079418834\n",
      "73 0.005355052689572728\n",
      "74 0.005268807908632713\n",
      "75 0.005185161424777912\n",
      "76 0.005103999849662486\n",
      "77 0.005025216230428708\n",
      "78 0.0049487096022712\n",
      "79 0.004874384577683492\n",
      "80 0.004802150968932554\n",
      "81 0.004731923440674459\n",
      "82 0.0046636211899467265\n",
      "83 0.004597167651058611\n",
      "84 0.004532490223153985\n",
      "85 0.004469520018444721\n",
      "86 0.004408191629312801\n",
      "87 0.004348442912655278\n",
      "88 0.004290214790005235\n",
      "89 0.004233451062101486\n",
      "90 0.004178098236706598\n",
      "91 0.004124105368584645\n",
      "92 0.004071423910651284\n",
      "93 0.004020007575398867\n",
      "94 0.003969812205781215\n",
      "95 0.003920795654814583\n",
      "96 0.0038729176732186323\n",
      "97 0.0038261398044790204\n",
      "98 0.003780425286768045\n",
      "99 0.003735738961207235\n",
      "100 0.0036920471860002156\n",
      "101 0.003649317756003167\n",
      "102 0.0036075198273368926\n",
      "103 0.0035666238466763762\n",
      "104 0.0035266014848838265\n",
      "105 0.003487425574678085\n",
      "106 0.003449070052057593\n",
      "107 0.0034115099012167155\n",
      "108 0.0033747211027153395\n",
      "109 0.003338680584680639\n",
      "110 0.0033033661768365674\n",
      "111 0.0032687565671722816\n",
      "112 0.00323483126107505\n",
      "113 0.0032015705427661167\n",
      "114 0.0031689554388900014\n",
      "115 0.0031369676841186854\n",
      "116 0.0031055896886420737\n",
      "117 0.003074804507425676\n",
      "118 0.0030445958111247184\n",
      "119 0.003014947858551726\n",
      "120 0.0029858454706020894\n",
      "121 0.0029572740055484317\n",
      "122 0.002929219335621129\n",
      "123 0.002901667824797618\n",
      "124 0.0028746063077285363\n",
      "125 0.0028480220697337808\n",
      "126 0.002821902827805713\n",
      "127 0.0027962367125609977\n",
      "128 0.002771012251086479\n",
      "129 0.0027462183506280907\n",
      "130 0.002721844283074805\n",
      "131 0.002697879670193238\n",
      "132 0.0026743144695706325\n",
      "133 0.0026511389612273173\n",
      "134 0.002628343734861623\n",
      "135 0.0026059196776931064\n",
      "136 0.0025838579628712013\n",
      "137 0.0025621500384194616\n",
      "138 0.0025407876166864754\n",
      "139 0.002519762664276823\n",
      "140 0.0024990673924368285\n",
      "141 0.0024786942478713216\n",
      "142 0.002458635903969293\n",
      "143 0.0024388852524172017\n",
      "144 0.002419435395180282\n",
      "145 0.0024002796368332697\n",
      "146 0.002381411477222743\n",
      "147 0.002362824604444781\n",
      "148 0.0023445128881222025\n",
      "149 0.0023264703729665246\n",
      "150 0.002308691272610773\n",
      "151 0.002291169963700176\n",
      "152 0.002273900980227914\n",
      "153 0.002256879008104575\n",
      "154 0.0022400988799498403\n",
      "155 0.002223555570096196\n",
      "156 0.002207244189794474\n",
      "157 0.0021911599826118515\n",
      "158 0.002175298320013607\n",
      "159 0.002159654697119727\n",
      "160 0.002144224728628772\n",
      "161 0.0021290041449012202\n",
      "162 0.002113988788194984\n",
      "163 0.002099174609046473\n",
      "164 0.0020845576627905897\n",
      "165 0.0020701341062133428\n",
      "166 0.002055900194331715\n",
      "167 0.002041852277294532\n",
      "168 0.0020279867973996488\n",
      "169 0.002014300286222076\n",
      "170 0.002000789361848467\n",
      "171 0.001987450726213195\n",
      "172 0.0019742811625320174\n",
      "173 0.001961277532828868\n",
      "174 0.0019484367755520802\n",
      "175 0.0019357559032762669\n",
      "176 0.0019232320004861781\n",
      "177 0.0019108622214394477\n",
      "178 0.0018986437881045308\n",
      "179 0.001886573988171227\n",
      "180 0.001874650173130586\n",
      "181 0.001862869756421377\n",
      "182 0.0018512302116406468\n",
      "183 0.0018397290708155923\n",
      "184 0.001828363922734442\n",
      "185 0.0018171324113339942\n",
      "186 0.0018060322341415908\n",
      "187 0.001795061140769273\n",
      "188 0.001784216931458461\n",
      "189 0.0017734974556726363\n",
      "190 0.0017629006107367556\n",
      "191 0.0017524243405212575\n",
      "192 0.0017420666341690647\n",
      "193 0.0017318255248639607\n",
      "194 0.00172169908863883\n",
      "195 0.001711685443222172\n",
      "196 0.001701782746921543\n",
      "197 0.0016919891975425178\n",
      "198 0.001682303031341911\n",
      "199 0.0016727225220140097\n"
     ]
    }
   ],
   "source": [
    "for k in range(200):\n",
    "\n",
    "    # forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((yout - ygt) ** 2 for ygt, yout in zip(ys, ypred))\n",
    "\n",
    "    # backward pass\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "    print(k, loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.936412200793154),\n",
       " Value(data=-0.911850210136414),\n",
       " Value(data=-0.9236982626413464),\n",
       " Value(data=0.9137314866500253)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
